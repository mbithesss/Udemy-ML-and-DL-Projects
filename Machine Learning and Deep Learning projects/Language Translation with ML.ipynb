{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5c66c6",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c14941d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input , LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915ff58-92c6-416c-9868-28c086b049f6",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06ed588-e34c-4689-823e-f24fe65604c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv(r\"C:\\Users\\SALOME\\Downloads\\Requirments (11)\\Hindi_English_Truncated_Corpus.csv\", encoding ='utf-8')\n",
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcb97b-d609-49c9-a9ff-a43e97d588b3",
   "metadata": {},
   "source": [
    "## Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e9efad7-ee25-4d58-89fb-62c81dad63ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22267528-ac72-4f27-8de6-00e3feb515ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127607 entries, 0 to 127606\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   source            127607 non-null  object\n",
      " 1   english_sentence  127605 non-null  object\n",
      " 2   hindi_sentence    127607 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "lines.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22015276-1f13-4d71-b070-cff33aea3dd4",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d853dbf6-4000-47f4-a625-8295c31dbba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source              0\n",
       "english_sentence    2\n",
       "hindi_sentence      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5737342-df4b-468c-b10d-42c12d8c2165",
   "metadata": {},
   "source": [
    "### Check for duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d809e4-302b-434b-9b76-eaf246391329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2778"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeaecab-5aba-443f-9f46-d69f00ee24fa",
   "metadata": {},
   "source": [
    "### Handle missing values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c1d34-0c1e-41da-a2cd-7a7b4d18d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[lines['source'] == 'ted']\n",
    "lines = lines[~pd.isnull(lines['english_sentence'])]\n",
    "lines.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e24d130-1c2d-46f7-80df-68ab1b8dabb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick any 25000 rows from the dataset\n",
    "lines = lines.sample(n=25000, random_state=42)\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdc1d8-b83a-4de3-b9f9-a2a72d4a6a88",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a05709-b596-428a-8cf9-80e4b42e0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case all characters in the dataset for simplicity\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.lower())\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a63e24b3-5ab7-440a-acf4-a39e963a7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes from the data\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dfa75d1-ec19-4742-809d-d62f3c4e100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "exclude = set(string.punctuation) #set of all special characters\n",
    "\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750caf50-358f-415a-af32-e2af1791d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers and extra spaces\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"२३०८१५७९४६\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.strip())\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.strip())\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'START_ ' + x + '_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68854ba5-bab7-4adf-bfa8-b9a63a868279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82040</th>\n",
       "      <td>ted</td>\n",
       "      <td>we still dont know who her parents are who she is</td>\n",
       "      <td>START_ हम अभी तक नहीं जानते हैं कि उसके मातापि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85038</th>\n",
       "      <td>ted</td>\n",
       "      <td>no keyboard</td>\n",
       "      <td>START_ कोई कुंजीपटल नहीं_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58018</th>\n",
       "      <td>ted</td>\n",
       "      <td>but as far as being a performer</td>\n",
       "      <td>START_ लेकिन एक कलाकार होने के साथ_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74470</th>\n",
       "      <td>ted</td>\n",
       "      <td>and this particular balloon</td>\n",
       "      <td>START_ और यह खास गुब्बारा_END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122330</th>\n",
       "      <td>ted</td>\n",
       "      <td>and its not as hard as you think integrate cli...</td>\n",
       "      <td>START_ और जितना आपको लगता है यह उतना कठिन नहीं...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                   english_sentence  \\\n",
       "82040     ted  we still dont know who her parents are who she is   \n",
       "85038     ted                                        no keyboard   \n",
       "58018     ted                    but as far as being a performer   \n",
       "74470     ted                        and this particular balloon   \n",
       "122330    ted  and its not as hard as you think integrate cli...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "82040   START_ हम अभी तक नहीं जानते हैं कि उसके मातापि...  \n",
       "85038                        START_ कोई कुंजीपटल नहीं_END  \n",
       "58018              START_ लेकिन एक कलाकार होने के साथ_END  \n",
       "74470                       START_ और यह खास गुब्बारा_END  \n",
       "122330  START_ और जितना आपको लगता है यह उतना कठिन नहीं...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e3dba-6c4a-4ec7-8ee7-91331685f374",
   "metadata": {},
   "source": [
    "### Create Vocabularies of unique English and Hindi words in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd3709d-55aa-4f12-8b82-a746f248221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sets to store unique words and populate them\n",
    "all_eng_words = set()\n",
    "for eng in lines['english_sentence']:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "all_hindi_words = set()\n",
    "for hin in lines['hindi_sentence']:\n",
    "    for word in hin.split():\n",
    "        if word not in all_hindi_words:\n",
    "            all_hindi_words.add(word)\n",
    "            \n",
    "# Calculate length of each English and Hindi sentence\n",
    "lines['length_eng_sentence'] = lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
    "lines['length_hindi_sentence'] = lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b73abf8-c797-4dd4-ad20-d73aca91f0b2",
   "metadata": {},
   "source": [
    "#### We filter out sentences that are longer than 20 words to simplify the training process and ensure that the model can handle the input within reasonable computational limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f2c673-d29f-4207-8c29-50c80fc573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out Sentences that are too long\n",
    "lines = lines[lines['length_eng_sentence'] <=20]\n",
    "lines = lines[lines['length_hindi_sentence'] <=20]\n",
    "\n",
    "# Check maximum length of remaining sentences\n",
    "max_length_src = max(lines['length_hindi_sentence'])\n",
    "max_length_tar = max(lines['length_eng_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e7a108-f0d6-4652-87cc-f2690b7bd1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>length_eng_sentence</th>\n",
       "      <th>length_hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82040</th>\n",
       "      <td>ted</td>\n",
       "      <td>we still dont know who her parents are who she is</td>\n",
       "      <td>START_ हम अभी तक नहीं जानते हैं कि उसके मातापि...</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85038</th>\n",
       "      <td>ted</td>\n",
       "      <td>no keyboard</td>\n",
       "      <td>START_ कोई कुंजीपटल नहीं_END</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58018</th>\n",
       "      <td>ted</td>\n",
       "      <td>but as far as being a performer</td>\n",
       "      <td>START_ लेकिन एक कलाकार होने के साथ_END</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74470</th>\n",
       "      <td>ted</td>\n",
       "      <td>and this particular balloon</td>\n",
       "      <td>START_ और यह खास गुब्बारा_END</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122330</th>\n",
       "      <td>ted</td>\n",
       "      <td>and its not as hard as you think integrate cli...</td>\n",
       "      <td>START_ और जितना आपको लगता है यह उतना कठिन नहीं...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source                                   english_sentence  \\\n",
       "82040     ted  we still dont know who her parents are who she is   \n",
       "85038     ted                                        no keyboard   \n",
       "58018     ted                    but as far as being a performer   \n",
       "74470     ted                        and this particular balloon   \n",
       "122330    ted  and its not as hard as you think integrate cli...   \n",
       "\n",
       "                                           hindi_sentence  \\\n",
       "82040   START_ हम अभी तक नहीं जानते हैं कि उसके मातापि...   \n",
       "85038                        START_ कोई कुंजीपटल नहीं_END   \n",
       "58018              START_ लेकिन एक कलाकार होने के साथ_END   \n",
       "74470                       START_ और यह खास गुब्बारा_END   \n",
       "122330  START_ और जितना आपको लगता है यह उतना कठिन नहीं...   \n",
       "\n",
       "        length_eng_sentence  length_hindi_sentence  \n",
       "82040                    11                     15  \n",
       "85038                     2                      4  \n",
       "58018                     7                      7  \n",
       "74470                     4                      5  \n",
       "122330                   16                     19  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51862f5-1bc9-4a88-be65-2eb05db2c7e1",
   "metadata": {},
   "source": [
    "## Concept of Encoding and Decoding in Machine Learning\n",
    "### Encoding\n",
    "Purpose: Convert human-readable text into a numerical format that a machine learning model can process.\n",
    "##### Process:\n",
    "##### Tokenization: Break down sentences into individual words or tokens.\n",
    "##### Indexing: Assign each unique word a unique number (index). \n",
    "##### Mapping: Create dictionaries to map words to their corresponding indices.\n",
    "##### Padding: Ensure all sequences (sentences) are the same length by adding special tokens (e.g., zero-padding) where necessary.\n",
    "\n",
    "### Decoding\n",
    "Purpose: Convert the numerical output of a machine learning model back into human-readable text.\n",
    "##### Process:\n",
    "##### Reverse Mapping: Use the dictionaries created during encoding to map indices back to their corresponding words.\n",
    "##### Sequence Generation: Form sentences by combining the words obtained from the reverse mapping.\n",
    "\n",
    "#### In this context:\n",
    "##### Encoder: The part of the model that processes the input sentence (e.g., English sentence) and converts it into a numerical format (encoded representation).\n",
    "##### Decoder: The part of the model that takes the encoded representation and generates the output sentence (e.g., Hindi sentence) in numerical format, which is then converted back to text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7281f9-dc0b-4be9-a2e3-b8280519e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort list of all unique English and Hindi words\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "\n",
    "# Calculate the number of unique tokens (words) in each language\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "\n",
    "# Ensure number of encoder tokens matches that of decoder tokens\n",
    "num_encoder_tokens = num_decoder_tokens\n",
    "num_decoder_tokens += 1 #for zero padding\n",
    "\n",
    "# Create a dictionary that maps each English word to a unique index\n",
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "\n",
    "# Create a dictionary that maps each Hindi word to a unique index\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create a reverse dictionary that maps each index back to the corresponding English word\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "\n",
    "# Create a reverse dictionary that maps each index back to the corresponding Hindi word\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "lines = shuffle(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c06e21-c906-4a10-975d-b7bc054f2e02",
   "metadata": {},
   "source": [
    "## Training model to translate English to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b962a346-7558-469b-8df7-f48bcad33bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to training and testing sets\n",
    "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "# Save Data to Pickle files\n",
    "X_train.to_pickle('X_train.pk1')\n",
    "X_test.to_pickle('X_test.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2c13e-0723-49db-9327-605f48259ca5",
   "metadata": {},
   "source": [
    "This function prepares batches of data for training by encoding input and output sequences into numerical formats (encoder_input_data, decoder_input_data, decoder_target_data). It handles tokenization, indexing, and one-hot encoding necessary for training a sequence-to-sequence model, ensuring each batch is ready for consumption by the neural network.\n",
    "\n",
    "The use of a generator allows for memory-efficient processing, especially useful for handling large datasets in machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7aa9ee42-9e91-4750-83b8-c602738f68cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, None, 300)            5935800   ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, None, 300)            5936100   ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               [(None, 300),                721200    ['embedding_4[0][0]']         \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               [(None, None, 300),          721200    ['embedding_5[0][0]',         \n",
      "                              (None, 300),                           'lstm_4[0][1]',              \n",
      "                              (None, 300)]                           'lstm_4[0][2]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 19787)          5955887   ['lstm_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19270187 (73.51 MB)\n",
      "Trainable params: 19270187 (73.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def generate_batch (X = X_train, y = y_train, batch_size = 128):\n",
    "    ''' Generate a batch of data '''''\n",
    "    while True:\n",
    "        for j in range (0, len(x), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] #decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "# Define the dimensionality of the latent space\n",
    "latent_dim = 300\n",
    "\n",
    "# Define encoder input layer\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Embedding layer for tokenizing and converting input sequences into dense vectors\n",
    "enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "# LSTM layer in the encoder to process input sequences and return states\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# Get encoder outputs (sequence), final hidden state, and final cell state\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# Keep only the states (final hidden and cell states) for later use in decoding\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder input layer\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# Embedding layer for tokenizing and converting decoder input sequences\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# LSTM layer in the decoder to generate output sequences and return states\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# Get decoder outputs (sequences), hidden state, and cell state using encoder states as initial states\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# Dense layer to predict probabilities over the target vocabulary\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the entire model that inputs encoder and decoder inputs and outputs decoder outputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with RMSprop optimizer and categorical crossentropy loss function\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Print model summary to show its architecture and parameters\n",
    "model.summary()\n",
    "\n",
    "# Calculate number of samples in training and validation sets\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "\n",
    "# Define batch size and number of epochs for training\n",
    "batch_size = 128\n",
    "epochs = 100     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01440a-5b57-4d2e-9047-228228efc067",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SALOME\\AppData\\Local\\Temp\\ipykernel_17952\\2930099277.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "155/155 [==============================] - 2158s 14s/step - loss: 7.6380 - val_loss: 7.0301\n",
      "Epoch 2/100\n",
      "155/155 [==============================] - 2057s 13s/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "155/155 [==============================] - 2140s 14s/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "155/155 [==============================] - 2484s 16s/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      " 13/155 [=>............................] - ETA: 54:23 - loss: nan"
     ]
    }
   ],
   "source": [
    "# Train the model using a generator function for batches of data\n",
    "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size, # Number of batches per epoch\n",
    "                    epochs=epochs, # Number of training epochs\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)\n",
    "\n",
    "# Save the weights of the trained model to a file\n",
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72a096-3c9f-4d2d-9899-ceae36d90349",
   "metadata": {},
   "source": [
    "### Final Decoder Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90800011-2564-4420-8bc5-f034529e73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "    \n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c9357-7244-4d53-a623-f9b536e406f8",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "Demonstrates evaluation of a sequence-to-sequence model trained for translating English sentences into Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47fb35-6794-431f-a873-74a64a38dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240cb5f-f066-459d-85fa-eb98f8dbb363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
